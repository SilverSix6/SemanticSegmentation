{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****This is the SVM Testing python notebook, everything needed to test results is contained within this notebook****\n",
   "id": "46351e3429e40992"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "from labeling.image_load_util import load_folder_image, get_full_path_from_root\n",
    "from utils.serialize import read_clusters, read_matrix\n",
    "import os\n",
    "\n",
    "# This cell takes the slic results and loads them for use\n",
    "matrix_file = open(\"data/slic/full_slic_matrix.bin\", \"rb\")\n",
    "cluster_file = open(\"data/slic/full_slic_cluster.bin\", \"rb\")\n",
    "print(\"Files loaded\")\n",
    "\n",
    "# Load the matrix and cluster\n",
    "matrix = read_matrix(matrix_file) # Matrix size image, pixel corresponds to cluster\n",
    "cluster = read_clusters(cluster_file) # Cluster ID, avg colour and centroid. \n",
    "\n",
    "image_directory = 'src/classification/data/raw/test-images/leftImg8bit/train'\n",
    "\n",
    "image_filenames =  load_folder_image(image_directory)\n",
    "\n",
    "id_directory = 'src/classification/data/raw/test-images/gtFine/train' # This is the ground truth labels\n",
    "search_string = 'label'  # Replace this with the substring you want to match\n",
    "id_filenames = []\n",
    "\n",
    "all_filenames = load_folder_image(id_directory)\n",
    "for id in all_filenames:\n",
    "    if search_string in str(id):\n",
    "        id_filenames.append(id)\n",
    "\n",
    "\n",
    "matrix_file.close()\n",
    "cluster_file.close()"
   ],
   "id": "bc90053197b51331"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from joblib import dump, load\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def extract_statistical_features(image):\n",
    "    # Split channels and compute mean, std, median, min, and max using list comprehension.\n",
    "    return np.array([\n",
    "        [np.mean(ch), np.std(ch), np.median(ch), np.min(ch), np.max(ch)]\n",
    "        for ch in cv2.split(image)\n",
    "    ]).flatten()\n",
    "\n",
    "def extract_features_from_clusters(cluster_matrix):\n",
    "    # Combine mean color and centroid for each cluster using list comprehension.\n",
    "    return np.array([\n",
    "        [cluster.l, cluster.a, cluster.b, cluster.x, cluster.y]\n",
    "        for cluster in cluster_matrix\n",
    "    ])\n",
    "\n",
    "def process_cluster(cluster, gt, image, slic):\n",
    "    # Computes all relevant features for cluster and the dominant ground truth label, fully modular.\n",
    "    cid = cluster.cid\n",
    "    mask = (slic == (cid + 1))\n",
    "    mask_uint8 = mask.astype(np.uint8) * 255\n",
    "    masked_image = cv2.bitwise_and(image, image, mask=mask_uint8)\n",
    "    stats = extract_statistical_features(masked_image)\n",
    "    feature_vector = np.concatenate((\n",
    "        [cluster.l, cluster.a, cluster.b, cluster.x, cluster.y],\n",
    "        stats\n",
    "    ))\n",
    "    labels, counts = np.unique(gt[mask], return_counts=True)\n",
    "    if counts.size == 0:\n",
    "        # Set a default label or skip processing as needed\n",
    "        dominant_label = -1\n",
    "    else:\n",
    "        dominant_label = labels[np.argmax(counts)]\n",
    "    return feature_vector, dominant_label\n",
    "\n",
    "def prepare_training_data(cluster_matrices, ground_truth_matrices, slic_matrix, images):\n",
    "    results = []\n",
    "    # Iterate over each image's data\n",
    "    for j, (clusters, gt, image, slic) in enumerate(zip(cluster_matrices, ground_truth_matrices, images, slic_matrix)):\n",
    "        # Parallel processing for all clusters in the current image\n",
    "        clusters_result = Parallel(n_jobs=-1)(\n",
    "            delayed(process_cluster)(cluster, gt, image, slic) for cluster in clusters\n",
    "        )\n",
    "        results.extend(clusters_result)\n",
    "    X_train, y_train = zip(*results)\n",
    "    return np.array(X_train), np.array(y_train)\n",
    "\n",
    "def train_svm(X_train, y_train):\n",
    "    # Create a pipeline with StandardScaler and SVC.\n",
    "    pipeline = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=10, gamma='scale', probability=True))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    return pipeline\n",
    "\n",
    "def process_cluster_pre(cluster, segments, image):\n",
    "    # Computes all relevant features for cluster, fully modular.\n",
    "    cid = cluster.cid\n",
    "    mask = (segments == (cid + 1))\n",
    "    mask_uint8 = mask.astype(np.uint8) * 255\n",
    "    masked_image = cv2.bitwise_and(image, image, mask=mask_uint8)\n",
    "    stats = extract_statistical_features(masked_image)\n",
    "    feature_vector = np.concatenate((\n",
    "        np.array([cluster.l, cluster.a, cluster.b, cluster.x, cluster.y]),\n",
    "        stats\n",
    "    ))\n",
    "    return cluster, feature_vector\n",
    "\n",
    "def predict_cluster_labels(cluster_matrix, model, segments, image):\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(process_cluster_pre)(cluster, segments, image) for cluster in cluster_matrix\n",
    "    )\n",
    "    clusters, features = zip(*results)\n",
    "    predictions = model.predict(features)\n",
    "    segmentation_result = np.zeros(segments.shape, dtype=np.int32)\n",
    "    for cluster, label in zip(clusters, predictions):\n",
    "        segmentation_result[segments == (cluster.cid + 1)] = label\n",
    "    return segmentation_result"
   ],
   "id": "f5bc33dd6c3d16ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cluster_cell = []\n",
    "matrix_cell = []\n",
    "matrix_file = open(\"data/slic/full_slic_matrix.bin\", \"rb\")\n",
    "cluster_file = open(\"data/slic/full_slic_cluster.bin\", \"rb\")\n",
    "\n",
    "\n",
    "for i in range(0,100):\n",
    "    cluster_cell.append(read_clusters(cluster_file))\n",
    "    matrix_cell.append(read_matrix(matrix_file))\n",
    "\n",
    "\n",
    "\n",
    "print(matrix_cell[0])\n",
    "print(cluster_cell[0][10])\n",
    "matrix_file.close()\n",
    "cluster_file.close()"
   ],
   "id": "a998045dfe6b87fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "id_directory = 'src/classification/data/raw/test-images/gtFine/train'\n",
    "id_directory_photos = 'src/classification/data/raw/test-images/leftImg8bit/train'\n",
    "# search_string = 'instance'  # Replace this with the substring you want to match\n",
    "search_string = 'label'  # Replace this with the substring you want to match\n",
    "id_filenames = []\n",
    "id_filenames_photos = []\n",
    "\n",
    "all_filenames = load_folder_image(id_directory)\n",
    "for id in all_filenames:\n",
    "    if search_string in str(id):\n",
    "        id_filenames.append(id)\n",
    "\n",
    "all_filenames = load_folder_image(id_directory_photos)\n",
    "for id in all_filenames:\n",
    "    id_filenames_photos.append(cv2.imread(id, cv2.IMREAD_COLOR))\n",
    "\n",
    "ground_truth_matrices = []\n",
    "for id_filename in id_filenames:\n",
    "    ground_truth_matrices.append(cv2.imread(id_filename, cv2.IMREAD_GRAYSCALE))"
   ],
   "id": "cfe2bdbd122947ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(np.shape(ground_truth_matrices))\n",
    "print(np.shape(id_filenames_photos))"
   ],
   "id": "4f6aae0ca31f3190"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "matrix_file = open(\"data/slic/full_slic_matrix.bin\", \"rb\")\n",
    "cluster_file = open(\"data/slic/full_slic_cluster.bin\", \"rb\")\n",
    "appended_gt = []\n",
    "matrix_cell = []\n",
    "cluster_cell = []\n",
    "image_cell = []\n",
    "\n",
    "\n",
    "for i in range(0,50):\n",
    "    cluster_cell.append(read_clusters(cluster_file))\n",
    "    matrix_cell.append(read_matrix(matrix_file))\n",
    "    appended_gt.append(ground_truth_matrices[i])\n",
    "    image_cell.append(id_filenames_photos[i])\n",
    "\n",
    "\n",
    "x_train, y_train = prepare_training_data(cluster_cell, appended_gt, matrix_cell,image_cell)\n",
    "matrix_file.close()\n",
    "cluster_file.close()"
   ],
   "id": "a709d649ab4d27f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# print(x_train)\n",
    "print(np.shape(x_train))"
   ],
   "id": "5771a0c4c027d121"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model = train_svm(x_train[0:2500], y_train[0:2500]) # Train on the first 2500 samples",
   "id": "fcb0b0827af3364e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_image = 30 # 9 at 77.5%, 11, 15,16, 3, 18, 22\n",
    "predicted_labels = predict_cluster_labels(cluster_cell[test_image], model, matrix_cell[test_image],id_filenames_photos[test_image])"
   ],
   "id": "cd51c2bb346bd48b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_matching_percentage(matrix1, matrix2):\n",
    "    # Ensure both matrices have the same shape\n",
    "    if np.shape(matrix1) != np.shape(matrix2):\n",
    "        raise ValueError(\"Matrices must have the same shape\")\n",
    "\n",
    "    # Compare the matrices element-wise\n",
    "    matching_elements = (matrix1 == matrix2)\n",
    "\n",
    "    # Calculate the percentage of matching values\n",
    "    matching_percentage = np.sum(matching_elements) / matching_elements.size * 100\n",
    "\n",
    "    return matching_percentage"
   ],
   "id": "12a6e1c285734c71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def label_consolidate(matrix):\n",
    "    matrix = np.where((7 <= matrix) & (matrix <= 10), 7, matrix)\n",
    "    matrix = np.where((11 <= matrix) & (matrix <= 16), 11, matrix)\n",
    "    matrix = np.where((17 <= matrix) & (matrix <= 20), 17, matrix)\n",
    "    matrix = np.where((21 <= matrix) & (matrix <= 22), 21, matrix)\n",
    "    matrix = np.where((24 <= matrix) & (matrix <= 25), 24, matrix)\n",
    "    matrix = np.where((matrix < 0) | (matrix > 25), 33, matrix)\n",
    "    return matrix\n",
    "\n",
    "def label_consolidate_colour(matrix):\n",
    "    rgb_image = np.zeros((matrix.shape[0], matrix.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    # Define masks\n",
    "    masks = [\n",
    "        ((0 <= matrix) & (matrix <= 6),       [111, 74, 0]),\n",
    "        ((7 <= matrix) & (matrix <= 10),      [52, 152, 219 ]),\n",
    "        ((11 <= matrix) & (matrix <= 16),     [22, 160, 133 ]),\n",
    "        ((17 <= matrix) & (matrix <= 20),     [243, 156, 18 ]),\n",
    "        ((21 <= matrix) & (matrix <= 22),     [249, 231, 159 ]),\n",
    "        ((24 <= matrix) & (matrix <= 25),     [165, 105, 189 ]),\n",
    "        ((matrix < 0) | (matrix > 25),        [169, 50, 38])\n",
    "    ]\n",
    "\n",
    "    # Assign colors\n",
    "    for mask, color in masks:\n",
    "        rgb_image[mask] = color\n",
    "\n",
    "    return rgb_image"
   ],
   "id": "a4e82d022c94afb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Less Restricted labels\n",
    "predicted_image = label_consolidate(predicted_labels)\n",
    "gt_image = label_consolidate(appended_gt[test_image])"
   ],
   "id": "26a8a90b07ed80df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "bounding_image = label_consolidate_colour(predicted_labels)\n",
    "image = id_filenames_photos[test_image]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image)  # Display original image\n",
    "plt.show()\n",
    "plt.figure(figsize=(8, 8))\n",
    "# Define custom legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor=np.array([111, 74, 0])/255,      label='Unlabelled'),\n",
    "    Patch(facecolor=np.array([52, 152, 219])/255,    label='Road/Ground'),\n",
    "    Patch(facecolor=np.array([22, 160, 133])/255,    label='Building/Wall'),\n",
    "    Patch(facecolor=np.array([243, 156, 18])/255,    label='Pole/Traffic Light'),\n",
    "    Patch(facecolor=np.array([249, 231, 159])/255,   label='Vegetation'),\n",
    "    Patch(facecolor=np.array([165, 105, 189])/255,   label='Person'),\n",
    "    Patch(facecolor=np.array([169, 50, 38])/255,     label='Vehicle')\n",
    "]\n",
    "\n",
    "# Display the legend\n",
    "plt.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.axis('off')  # Hide axis for cleaner visualization\n",
    "plt.tight_layout()\n",
    "plt.imshow(bounding_image, alpha=0.5)  # Overlay bounding image with 50% transparency\n",
    "plt.axis('off')  # Optional: Hide axis ticks\n",
    "plt.show()"
   ],
   "id": "681145c9efd38964"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "percentage = calculate_matching_percentage(predicted_image, gt_image)\n",
    "\n",
    "print(f\"Matching percentage: {percentage:.2f}%\")"
   ],
   "id": "ec99896229b97987"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from utils.segmentation_utils import segmented_to_color\n",
    "\n",
    "# Display Segmented image with color\n",
    "color_segmented_matrix = segmented_to_color(matrix_cell[test_image], cluster_cell[test_image])\n",
    "plt.imshow(color_segmented_matrix)\n",
    "plt.show()\n"
   ],
   "id": "e476c35d60194d50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(predicted_image, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.imshow(gt_image, cmap='gray')\n",
    "plt.colorbar()"
   ],
   "id": "4fbf2a8924875d95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "avg_accuracy = 0\n",
    "max_iter = 50\n",
    "\n",
    "# Define custom legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor=np.array([111, 74, 0])/255,      label='Unlabelled'),\n",
    "    Patch(facecolor=np.array([52, 152, 219])/255,    label='Road/Ground'),\n",
    "    Patch(facecolor=np.array([22, 160, 133])/255,    label='Building/Wall'),\n",
    "    Patch(facecolor=np.array([243, 156, 18])/255,    label='Pole/Traffic Light'),\n",
    "    Patch(facecolor=np.array([249, 231, 159])/255,   label='Vegetation'),\n",
    "    Patch(facecolor=np.array([165, 105, 189])/255,   label='Person'),\n",
    "    Patch(facecolor=np.array([169, 50, 38])/255,     label='Vehicle')\n",
    "]\n",
    "\n",
    "for i in range(0,max_iter):\n",
    "    predicted_labels = predict_cluster_labels(cluster_cell[i], model, matrix_cell[i], id_filenames_photos[i])\n",
    "    predicted_image = label_consolidate(predicted_labels)\n",
    "    bounding_image = label_consolidate_colour(predicted_labels)\n",
    "    gt_image_test = label_consolidate(appended_gt[i])\n",
    "    percentage = calculate_matching_percentage(predicted_image, gt_image_test)\n",
    "    avg_accuracy += percentage\n",
    "    print(f\"At iteration {i} matching percentage: {percentage:.2f}%\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(id_filenames_photos[i])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.axis('off')  # Hide axis for cleaner visualization\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(bounding_image)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "avg_accuracy = avg_accuracy/max_iter\n",
    "print(f\"Average accuracy: {avg_accuracy:.2f}%\")"
   ],
   "id": "80f8d66b05041a8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(avg_accuracy)",
   "id": "52216cd46d209b10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "842066373a456099"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
